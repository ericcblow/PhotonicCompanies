{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "726e61b1-f473-40a0-9bfe-b530b32d41f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install geopy\n",
    "# !pip install geopandas\n",
    "# !pip install basemap\n",
    "# !pip install folium\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4252068-c132-41b2-8902-6eaf232b808d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import random\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "009b3eb8-71e3-4518-bb4b-ed8e250ea480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_excel_data(file_path, sheet_name):\n",
    "    # Load the workbook and select the desired sheet\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook[sheet_name]\n",
    "\n",
    "    # Get labels from row 2 (B2 to G2)\n",
    "    labels = [sheet.cell(row=2, column=col).value for col in range(2, 7)]  # Columns B-G\n",
    "    # print(labels)\n",
    "    \n",
    "    # Check if all labels are present\n",
    "    if not all(labels):\n",
    "        raise ValueError(\"One or more labels in row 2 are missing.\")\n",
    "\n",
    "    # Initialize a dictionary to store the data for each label\n",
    "    data = {label: [] for label in labels}\n",
    "    \n",
    "    # Get data from rows starting at B3 (columns B-G)\n",
    "    for row in sheet.iter_rows(min_row=3, min_col=2, max_col=7, values_only=True):\n",
    "        for col_index, value in enumerate(row):\n",
    "            if value:  # Skip empty cells\n",
    "                data[labels[col_index]].append(value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d794b92f-ca18-4ae9-ac64-893162eedd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['LightMatter', 'Lightelligence', 'Ayar Labs', 'PsiQuantum', 'Quantum Transistors', 'Q.ANT', 'Lumai ', 'iPRONICS', 'Salience Labs', 'Pixel Photonics', 'Dream Photonics ', 'Akhetonics', 'Celestial AI', 'Xscape Photonics', 'Optelligence', 'Nubis Communications', 'Voyant Photonics', 'Aegiq', 'Nu-quantum', 'Miraex', 'Nordic Quantum Computing Group', 'Orca Computing', 'Photonic Inc', 'Quandela ', 'Quix', 'Sparrow Quantum', 'Lumiphase', 'Enosemi', 'Nexus Photonics', 'Black Semiconductor'], 'Size': [200, 200, '200+', 280, 50, 100, 20, '< 50', '< 50', '< 50', 10, '< 50', '< 200', '< 50', '< 3', '< 50 ', '< 30', '< 50 ', '< 50 ', 50, 'CLOSED 11/2024', '~100', '< 50', '< 50', '< 50', '< 50 ', '< 100 ', '< 50 ', '< 20 ', '<200'], 'Focus': ['Optical MVM / PNN', 'Optical MVM / Networking', 'Optical I/O for AI, Interconnection ', 'Quantum Computing / Optical Interconnect', 'solid-state\\xa0quantum\\xa0processor', 'Optical MVM', '3D optical AI computing ', 'Microwave Photonics and Networking', 'silicon photonic solutions that will solve the challenge the growth in AI', 'Superconducting Nanowire Single Photon Detectors (SNSPD) and Quantum Computing', 'Packaging and Layout Design; world’s best chipsets for integrated photonics', 'Neuromorphic, Quantum, Analog, Digital, ADC', 'Photonic Fabric (Networking for AI)', 'photonics solutions that enable the next generation of AI, ML, and simulation hardware.', 'Photonic AI ', 'Co-Packaged Optics, Pluggable Transceivers, Active Optical Cables', 'LiDAR', 'Full Stack Quantum Computing', 'Photonic-Enabled Quantum Computing', 'photonic and quantum solutions for next-generation sensing, networking and computing', 'CLOSED 11/2024', 'error-corrected photonic quantum computers', 'Silicon photonic Quantum Computing', 'Silicon photonic Quantum Computing', 'photonic quantum processor based upon silicon nitride waveguides', 'photonic quantum technology components', 'Optical interconnects and computing using Si Ph and BTO', 'Photonic-Electronic Design IP and Custom Silicon', 'Advanced heterogeneous integration, SiN, GaAs, GaN, InP', '2D material graphene to create ultra-fast, energy-efficient, and scalable chip fabrics, Graphene-based photonic modulator / detector '], 'CEO/Founders/Affliation': ['Nick Harris, Dirk Englund, David Miller, MIT, Stanford', 'Yichen Shen, MIT ', 'Mark Wade', \"Jeremy O'Brien;\\xa0Terry Rudolph;\\xa0Peter Shadbolt; Mark Thompson\", 'Shmuel Bachinsky', 'Michael Förtsch, TRUMPF Photonics', 'James Spall, University of Oxford', 'Jose Capmany', 'Oxford, Harish Bhaskaran and Wolfram Pernice', 'Wolfram Pernice', 'Lukas Chrostowski', 'Michael Kissner, \\nLeonardo Del Bino', 'David Lazovsky', 'Vivek Raghunathan, Michal Lipson, Keren Bergman, Columbia', 'Volker J. Sorger', 'Peter Winzer, Bell Labs', 'Steven Miller, Michel Lipson', 'Scott Dufferwiel, Univ. of Sheffield', 'Dr. Carmen Palacios-Berraquero, University of Cambridge', 'Nicolas Abelé, Karel Dumon and Clément Javerzac-Galy, Miraex, EPFL ', 'CLOSED 11/2024', 'Professor Ian Walmsley, Josh Nunn, Richard Murray, University of Oxford', 'Paul Terry, Stephanie Simmons, Simon Fraser University', 'Centre for Nanoscience and Nanotechnology', 'Hans van den Vlekkert, University of Twente', 'Peter Lodahl, Niels Bohr Institute Quantum Photonics Lab', 'Stefan Abel, BTO Modulator Author,  started by IBM Si Ph people', 'Matt Streshinsky, Came out of Lumious Computing', 'Tin Komljenovic, Chong Zhang, John Bowers, UCSB', ' Daniel Schall', 'x'], 'Location': ['Mountain View, CA and Toronto, Canada, and Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel and NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancouver, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'New York, NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, UK and Toronto, Canada, and Austin, Tx, USA', 'Vancouver, BC, Canada', 'Massy, Île-de-France', 'Ulm, Germany and Stuttgart Germany, and Amsterdam, Netherlands, and Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Startup'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_SU = import_excel_data(file_path, sheet_name)\n",
    "    print(data_SU)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "992f9d63-498d-4af5-965d-36ef0ca6c7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Nvidia Research', 'Google ', 'NEC', 'Intel ', 'Universal Display Corporation', 'OpenAI', 'Apple', 'AMD', 'Cisco', 'Luxtera', 'Infinera ', 'NeoPhotonics', 'Coherent Inc. ', 'Hamamatsu Corporation', 'M Squared\\xa0', 'NTT', 'IBM', 'Synopsys Photonic Solutions'], 'Size': ['200+', '300+', '70+', 700, 470, 500, 100], 'Focus': ['Circuit Research Group ', 'Google Quantum AI + Taara @ Google X + Google Platforms', 'Fiber Sensing, Optical Compute, Fiber Telecommunication Optimization', 'Optical input/output (I/O) technology innovation', 'OLED technology, Tangential interest in optical compute', 'quantum technology, biophotonics, and chemical sensing'], 'CEO/Founders/Affliation': ['Tom Gray: Head of CRG', 'Quantum: Hartmut Neven, Taara: Mahesh Krishnaswamy ', 'President: Chris White, ONS Head: Ting Wang, Princeton University', 'Richard Uhlig', 'Sherwin Seligsohn, Stephen Forrest, Princeton University', 'GRAEME MALCOLM OBE, Strathclyde University'], 'Location': ['Santa Clara, California and Seattle, Washington ', 'Mountain View, California and Zürich, Switzerland and Seattle, Washington and San Francisco, CA', 'Princeton, NJ', 'Santa Clara, California and Hillsboro, Oregon ', 'Ewing Township, NJ', 'NJ, USA and Japan', 'Glasgow and London and Palo Alto and  Boston and Berlin', 'Zurich, Switzerland', 'Mountain View, California']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Large Company'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_LC = import_excel_data(file_path, sheet_name)\n",
    "    print(data_LC)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba18db43-69da-46a7-a641-94178ecdc28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Global Foundries', 'AIM Photonics ', 'Advanced Micro Foundry (AMF) ', 'HHI ', 'Smart Photonics', 'LioniX', 'Tower Semiconductor', 'LIGENTEC', 'VTT', 'IMEC', 'CMC Microsystems', 'X-Fab', 'JePPiX'], 'Size': [13000, 500, 130, '300\\xa0', '190\\xa0', 70, 10000, 100, '2,355\\xa0', 5500, '103\\xa0', 4500, 20], 'Focus': [' FDX™ FD-SOI, Silicon Photonics, RF SOI, BCD & BCDLite, Bulk CMOS, FinFET 12LP+, SiGe', 'Active/Passive Si, Electronic Interposer, Low-Loss Active Si, Quantum Flex, SiN, Active Interposer, III-V, SiN Visible  ', 'SOI, 800nm SiN on SOI, CoPackaging,  ', 'MPW, InP, RF PIC', 'MPW, InP ', 'SiN TriPleX , Visible, 1550nm, MEMs', '200mm Si, SiN, Darpa LUMOS,  RF, CMOS, MEMS', 'All-Nitride-Core Photonics, ', 'TFLN, Hybrid Integration, Heterogenenous Intergration, High depth Si, Packaging', '200nm CMOS+Si, 300nm CMOS+Si, MPW, Volume, SiN, Packaging, Research', 'Canadian consortium for integrated photonics and packaging', '350nm - 130nm CMOS, RF MEMS,  SIC, GaN, Microfluidics', 'European consortium for integrated photonics'], 'CEO/Founders/Affliation': ['CEO: Thomas Caulfield and Si Ph lead: Kevin Soukup', 'CEO Wade Cook, Ph  Dir: Chris Baiocco, SUNY', 'CEO Jagadish CV', 'Head of Ph  Klemens Janiak. ', 'CEO Johan Feenstra, VP R&D Yuval Kariv', 'CEO Arne Leinse, CTO Ronald Dekker,', 'CEO Russell C. Ellwanger, ', 'Michael Geiselmann, Michael Zervas,  Tobias Kippenberg,  EPFL ', 'CEO Antti Vasara', 'CEO Luc Van den hove and CTO Ilan Spillinger', 'CEO: Gordon Harling', 'CEO: Rudi De Winter, ', 'CEO: Francisco Rodrigues'], 'Location': ['Austin, Texas and Dallas, Texas and Essex Junction, Vermont and Malta, New York and New York, New York and Santa Clara, California and San Diego, California and Dresden, Germany and Leuven Belgium and Munich, Germany and Sofia, Bulgaria and Bengaluru, India and Hsinchu, Taiwan and Malaysia and Shanghai and Seoul, South Korea and Singapore and Yokohama, Japan', '\\tAlbany, NY and Rochester, NY', 'Singapore ', 'Berlin, Germany and Goslar, Germany', 'Eindhoven,  Netherlands', 'Enschede, Netherlands', 'Migdal Haemek, Israel and Newport Beach, CA, USA and  San Antonio, Texas and Hokuriku, Japan and Agrate, Italy', 'Vaud, Switzerland', 'Otaniemi Espoo Finland', 'Leuven, Belgium and San Francisco, CA and Kissimmee, FL and Berkeley, CA and San Jose, CA and Eindhoven, Netherlands, and Antwerp, Netherlands and Chicago, IL, USA, and Ann Harbor, Michigan, and West Lafayette, IN ', 'Montreal, Canada and Kingston, Canada and Ottawa, Canada', 'Lubbock, USA and Corbeil-Essonnes, France and Itzehoe, Germany and Dresden Germany, and Erfurt Germany, and Kuching, Malaysia', 'Eindhoven, Netherlands']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Foundries'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_F = import_excel_data(file_path, sheet_name)\n",
    "    print(data_F)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "979d4eba-50eb-4cd6-a425-53bcc01d1a43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Bascom Hunter', 'Photonic Systems Inc', 'Phase Sensitive Innovations'], 'Size': ['< 100', '< 100', '< 100'], 'Focus': ['Microwave Photonics, Neuromorphioc Photonics', 'Microwave Photonics, Interference Cancellation', 'Microwave Photonics, Diffractive Optics, Phase Arrays, Thin Film LiNo'], 'CEO/Founders/Affliation': ['Paul R. Prucnal , Andy McCandless', 'Charles Cox, Edward Ackerman', 'Dennis Prather, Univ of Delware '], 'Location': ['Baton Rouge, LA and St. Louis, MO', 'Billerica, MA', 'Newark, DE, USA']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Defense Contractors'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_DC = import_excel_data(file_path, sheet_name)\n",
    "    print(data_DC)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8ce73-2611-4a6e-8617-63be6401238b",
   "metadata": {},
   "source": [
    "### If a company has multiple locations splits that into new rows and then duplicate the information to the new rows, the deliminator is \" and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6312641-e84e-440c-8091-21ce1e81f0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword = \" and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb27b6ad-39ac-47f6-b998-7ceb3c7b4da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search and split\n",
    "def search_and_split(Locations, keyword):\n",
    "    results = []\n",
    "    split_strings = []\n",
    "    NewLocations = []\n",
    "    Indx2Dup = []\n",
    "    j=0\n",
    "    NewLocations = Locations\n",
    "    for idx, string in enumerate(Locations):\n",
    "        # print(idx)\n",
    "        start_idx = string.find(keyword)\n",
    "        if start_idx != -1:  # Keyword found\n",
    "            end_idx = start_idx + len(keyword) - 1\n",
    "            results.append((idx, start_idx, end_idx))\n",
    "\n",
    "            # Split the string into two parts: before and after the keyword\n",
    "            before_keyword = string[:start_idx].strip()\n",
    "            after_keyword = string[end_idx + 1:].strip()\n",
    "            split_strings.append((before_keyword, after_keyword))\n",
    "            \n",
    "            # Append and replace new split string into original list\n",
    "            # Conditionals are to look for blank spaces or punctions and not include them \n",
    "            threshold = 2\n",
    "            if len(before_keyword) > threshold and len(after_keyword) > threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = before_keyword\n",
    "                NewLocations.insert(idx+1, after_keyword)\n",
    "                #idx is no longer real as New List grows, need to keep track\n",
    "                Indx2Dup.append(idx-j)\n",
    "                j+=1\n",
    "                #When duplicating the rows if there are consecutive number dup from \n",
    "                    # row - number of consecutive numbers\n",
    "                \n",
    "            if len(before_keyword) <= threshold and len(after_keyword) > threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = after_keyword\n",
    "                \n",
    "            if len(before_keyword) > threshold and len(after_keyword) <= threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = before_keyword            \n",
    "\n",
    "                \n",
    "    return results, split_strings, NewLocations, Indx2Dup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b78446a3-10b3-474e-abdd-6d908001967a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate: \n",
    "def duplicate_values_with_repeated_indices(indices, source_list):\n",
    "    # Create a new list to store the results\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(source_list)):\n",
    "        count = indices.count(i)  # Count occurrences of the index in the indices array\n",
    "        result.extend([source_list[i]] * (count + 1))  # Original + count of duplicates\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b2906f6-1311-40e9-88d3-64fdfdb9eede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location List:\n",
      "['Mountain View, CA and Toronto, Canada, and Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel and NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancouver, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'New York, NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, UK and Toronto, Canada, and Austin, Tx, USA', 'Vancouver, BC, Canada', 'Massy, Île-de-France', 'Ulm, Germany and Stuttgart Germany, and Amsterdam, Netherlands, and Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n",
      "30\n",
      "\n",
      "New Location List:\n",
      "['Mountain View, CA', 'Toronto, Canada,', 'Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel', 'NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancouver, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'New York, NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, UK', 'Toronto, Canada,', 'Austin, Tx, USA', 'Vancouver, BC, Canada', 'Massy, Île-de-France', 'Ulm, Germany', 'Stuttgart Germany,', 'Amsterdam, Netherlands,', 'Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n",
      "38\n",
      "\n",
      "Index to Duplicate\n",
      "[0, 0, 4, 21, 21, 24, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "# Testing new search and split function\n",
    "# strings = [\n",
    "#     \"This is a sample string and it contains and perhaps denver and.\",\n",
    "#     \"Another example without the keyword.\",\n",
    "#     \"and is a conjunction.\",\n",
    "#     \"Nothing here or here\",\n",
    "#     \"But maybe here and there\"\n",
    "# ]\n",
    "\n",
    "# # Search for the keyword \"and\" and split strings\n",
    "# keyword = \"and\" # and is in Netherlands hahaha\n",
    "# matches, splits, nLocations, Indx = search_and_split(strings, keyword)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Keyword Matches (Start and End Indices):\")\n",
    "# for match in matches:\n",
    "#     print(f\"String index: {match[0]}, Start index: {match[1]}, End index: {match[2]}\")\n",
    "\n",
    "# print(\"\\nSplit Strings:\")\n",
    "# for idx, (before, after) in enumerate(splits):\n",
    "#     print(f\"Original String Index: {matches[idx][0]}\")\n",
    "#     print(f\"Before: '{before}'\")\n",
    "#     print(f\"After: '{after}'\")\n",
    "    \n",
    "# # print(\"\\nResult\")\n",
    "# # print(nLocations)\n",
    "\n",
    "print(\"\\nLocation List:\")\n",
    "print(data_SU[\"Location\"])\n",
    "print(len(data_SU[\"Location\"]))\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_SU[\"Location\"], keyword)\n",
    "\n",
    "print(\"\\nNew Location List:\")\n",
    "print(nLocations)\n",
    "print(len(nLocations))\n",
    "\n",
    "print(\"\\nIndex to Duplicate\")\n",
    "print(Indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "06029d19-a774-4f3c-afa3-9ead4e68642e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_SU.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc5aa8e8-dfe5-4d9a-8693-4d272c9d1271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# indices = [1, 1, 3, 3, 3, 5]  # Indices for duplication\n",
    "# source_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "\n",
    "# # Duplicate values based on repeated indices\n",
    "# result = duplicate_values_with_repeated_indices(indices, source_list)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19957a0b-a496-4960-b870-9b55a4b7229f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate values at the specified indices for start up tab\n",
    "data_SU['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_SU['Company Name'])\n",
    "data_SU['Size'] = duplicate_values_with_repeated_indices(Indx, data_SU['Size'])\n",
    "data_SU['Focus'] = duplicate_values_with_repeated_indices(Indx, data_SU['Focus'])\n",
    "data_SU['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_SU['CEO/Founders/Affliation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0defbef2-4e1a-4e3b-b81b-96cc1065b9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same for LC, F, and DC\n",
    "matches, splits, nLocations, Indx = search_and_split(data_LC[\"Location\"], keyword)\n",
    "data_LC['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_LC['Company Name'])\n",
    "data_LC['Size'] = duplicate_values_with_repeated_indices(Indx, data_LC['Size'])\n",
    "data_LC['Focus'] = duplicate_values_with_repeated_indices(Indx, data_LC['Focus'])\n",
    "data_LC['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_LC['CEO/Founders/Affliation'])\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_F[\"Location\"], keyword)\n",
    "data_F['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_F['Company Name'])\n",
    "data_F['Size'] = duplicate_values_with_repeated_indices(Indx, data_F['Size'])\n",
    "data_F['Focus'] = duplicate_values_with_repeated_indices(Indx, data_F['Focus'])\n",
    "data_F['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_F['CEO/Founders/Affliation'])\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_DC[\"Location\"], keyword)\n",
    "data_DC['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_DC['Company Name'])\n",
    "data_DC['Size'] = duplicate_values_with_repeated_indices(Indx, data_DC['Size'])\n",
    "data_DC['Focus'] = duplicate_values_with_repeated_indices(Indx, data_DC['Focus'])\n",
    "data_DC['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_DC['CEO/Founders/Affliation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7aa1c6-862d-4ae8-8d51-4d8f0a0aa367",
   "metadata": {},
   "source": [
    "### Now I have corrected list for companies with multiple location, we convert rough individual locations into coordinates and make a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c32be4a0-c69c-4e06-aba2-0fd2dea1ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Rough Location to longitude and latitude\n",
    "\n",
    "def get_coordinates_with_duplicates(locations, w_Debug=False):\n",
    "    # Initialize the geolocator\n",
    "    geolocator = Nominatim(user_agent=\"location_to_coordinates\")\n",
    "    cache = {}  # Cache to store already-queried locations\n",
    "    results = []  # Final list of results\n",
    "\n",
    "    for location in locations:\n",
    "        tries = 3\n",
    "        if location in cache:\n",
    "            # Use cached result if available\n",
    "            results.append(cache[location])\n",
    "        else:\n",
    "            try:\n",
    "                # Geocode the location\n",
    "                if w_Debug == True:\n",
    "                    print(location)\n",
    "                for i in range(tries):\n",
    "                    try:\n",
    "                        loc = geolocator.geocode(location)\n",
    "                    except KeyError as e:\n",
    "                        if i < tries - 1: # i is zero indexed\n",
    "                            print(Location, \"failed, trying again\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                if loc:\n",
    "                    coordinates = (loc.latitude, loc.longitude)\n",
    "                else:\n",
    "                    coordinates = None\n",
    "                    print('Could find coordinates for ',location, '.')\n",
    "                    print('Please change the location name within the database.\\n')\n",
    "                # Cache the result and append to results\n",
    "                cache[location] = coordinates\n",
    "                results.append(coordinates)\n",
    "            except GeocoderTimedOut:\n",
    "                # Handle timeout gracefully\n",
    "                results.append(None)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd54006c-3a85-4831-87c3-379a8021ed94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mountain View, CA', 'Toronto, Canada,', 'Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel', 'NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancouver, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'New York, NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, UK', 'Toronto, Canada,', 'Austin, Tx, USA', 'Vancouver, BC, Canada', 'Massy, Île-de-France', 'Ulm, Germany', 'Stuttgart Germany,', 'Amsterdam, Netherlands,', 'Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n"
     ]
    }
   ],
   "source": [
    "print(data_SU[\"Location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d999d-502c-4f4b-9081-d9a0f3682a07",
   "metadata": {},
   "source": [
    "### Check here if any rough addresses aren't able to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c080c33-590b-43a9-ae4c-2fdccd367aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get coordinates from location list \n",
    "# Sometimes fails rerun\n",
    "coordinates_SU = get_coordinates_with_duplicates(data_SU[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7cce5d-0bd6-44cb-9b75-de31b60a5f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates_LC = get_coordinates_with_duplicates(data_LC[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "faf6b945-f03f-4a8c-aa48-d931d3c27410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin, Texas\n",
      "Dallas, Texas\n",
      "Essex Junction, Vermont\n",
      "Malta, New York\n",
      "New York, New York\n",
      "Santa Clara, California\n",
      "San Diego, California\n",
      "Dresden, Germany\n",
      "Leuven Belgium\n",
      "Munich, Germany\n",
      "Sofia, Bulgaria\n",
      "Bengaluru, India\n",
      "Hsinchu, Taiwan\n",
      "Malaysia\n",
      "Shanghai\n"
     ]
    },
    {
     "ename": "GeocoderUnavailable",
     "evalue": "HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Shanghai&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:357\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[0;32m    359\u001b[0m     )\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:826\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    823\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    825\u001b[0m     )\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    827\u001b[0m         method,\n\u001b[0;32m    828\u001b[0m         url,\n\u001b[0;32m    829\u001b[0m         body,\n\u001b[0;32m    830\u001b[0m         headers,\n\u001b[0;32m    831\u001b[0m         retries,\n\u001b[0;32m    832\u001b[0m         redirect,\n\u001b[0;32m    833\u001b[0m         assert_same_host,\n\u001b[0;32m    834\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    835\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    836\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    837\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    838\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[0;32m    840\u001b[0m     )\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:826\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    823\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    825\u001b[0m     )\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    827\u001b[0m         method,\n\u001b[0;32m    828\u001b[0m         url,\n\u001b[0;32m    829\u001b[0m         body,\n\u001b[0;32m    830\u001b[0m         headers,\n\u001b[0;32m    831\u001b[0m         retries,\n\u001b[0;32m    832\u001b[0m         redirect,\n\u001b[0;32m    833\u001b[0m         assert_same_host,\n\u001b[0;32m    834\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    835\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    836\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    837\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    838\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[0;32m    840\u001b[0m     )\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Shanghai&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopy\\adapters.py:482\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 482\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Shanghai&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m coordinates_F \u001b[38;5;241m=\u001b[39m get_coordinates_with_duplicates(data_F[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[92], line 21\u001b[0m, in \u001b[0;36mget_coordinates_with_duplicates\u001b[1;34m(locations, w_Debug)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tries):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         loc \u001b[38;5;241m=\u001b[39m geolocator\u001b[38;5;241m.\u001b[39mgeocode(location)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m tries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# i is zero indexed\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py:297\u001b[0m, in \u001b[0;36mNominatim.geocode\u001b[1;34m(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)\u001b[0m\n\u001b[0;32m    295\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.geocode: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, url)\n\u001b[0;32m    296\u001b[0m callback \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_json, exactly_one\u001b[38;5;241m=\u001b[39mexactly_one)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_geocoder(url, callback, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopy\\geocoders\\base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[1;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_json:\n\u001b[1;32m--> 368\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter\u001b[38;5;241m.\u001b[39mget_json(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mreq_headers)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter\u001b[38;5;241m.\u001b[39mget_text(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mreq_headers)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopy\\adapters.py:472\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m, timeout, headers):\n\u001b[1;32m--> 472\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopy\\adapters.py:494\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[1;34m(self, url, timeout, headers)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GeocoderServiceError(message)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GeocoderUnavailable(message)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, requests\u001b[38;5;241m.\u001b[39mTimeout):\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GeocoderTimedOut(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mGeocoderUnavailable\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Shanghai&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))"
     ]
    }
   ],
   "source": [
    "coordinates_F = get_coordinates_with_duplicates(data_F[\"Location\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c53428d-df95-418c-a714-066979e257bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31.2312707, 121.4700152)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates_with_duplicates([\"Shanghai, \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37da191-1e4b-4ca7-a16f-6a21a1fcdcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates_DC = get_coordinates_with_duplicates(data_DC[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e2fec-2db1-4da2-aea3-f944f5c75a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example list of locations\n",
    "# locations = [\n",
    "#     \"Boston, MA, USA\",\n",
    "#     \"New York, NY, USA\",\n",
    "#     \"Boston, MA, USA\",  # Duplicate\n",
    "#     \"San Francisco, CA, USA\",\n",
    "#     \"New York, NY, USA\"  # Duplicate\n",
    "# ]\n",
    "\n",
    "# # Get coordinates\n",
    "# coordinates = get_coordinates_with_duplicates(locations)\n",
    "\n",
    "# # Print results\n",
    "# for location, coord in zip(locations, coordinates):\n",
    "#     if coord:\n",
    "#         print(f\"{location}: Latitude = {coord[0]}, Longitude = {coord[1]}\")\n",
    "#     else:\n",
    "#         print(f\"{location}: Coordinates not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487d8d1-a4ce-4f94-b24b-9c1c6730f149",
   "metadata": {},
   "source": [
    "### Cost-of-living "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ed503-6792-4b19-9efb-169e384cad75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maybe scrape from Numbeo\n",
    "# Kinda difficult to find an easy dataset for this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960af8-e9a9-47c9-9882-20f6ae1be699",
   "metadata": {},
   "source": [
    "### Plot coordinates on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7fecb-af40-4487-b111-b7bdaad08ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python-visualization.github.io/folium/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2b578-00b3-40a0-b2ee-e99a334d9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_pair():\n",
    "    pairs = []\n",
    "    Variation_Magnitiude = 0.02\n",
    "    Exclude_Zone = 0.01\n",
    "    while True:\n",
    "        num1 = random.uniform(-Variation_Magnitiude, Variation_Magnitiude)\n",
    "        num2 = random.uniform(-Variation_Magnitiude, Variation_Magnitiude)\n",
    "        if not (-Exclude_Zone <= num1 <= Exclude_Zone or -Exclude_Zone <= num2 <= Exclude_Zone):\n",
    "            pairs = [num1, num2]\n",
    "            break\n",
    "    return pairs\n",
    "\n",
    "# # Example usage\n",
    "# random_pair = generate_random_pair()\n",
    "# print(f\"Generated random pair: {random_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60f829-f7fa-42f9-829d-acb6d28004da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example using folium \n",
    "# m = folium.Map(location=[41,29])\n",
    "\n",
    "# # folium.Circle(\n",
    "# #     radius=5000,\n",
    "# #     location=[41,29],\n",
    "# #     color='crimson',\n",
    "# #     fill=False,).add_to(m)\n",
    "\n",
    "# # folium.CircleMarker(location=(41,29),radius=100, fill_color='red').add_to(m)\n",
    "\n",
    "# folium.Marker(location=[41,29], popup =  'Sakarya').add_to(m)\n",
    "# new_pair = generate_random_pair()\n",
    "# folium.Marker(location=[41+new_pair[0],29+new_pair[1]], popup =  'Sakarya').add_to(m)\n",
    "\n",
    "# minimap = plugins.MiniMap()\n",
    "# m.add_child(minimap)\n",
    "\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30de87-db72-404f-be6a-4aeff1398367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac1f08-da2c-458a-9d81-ceec2f2e4704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting SUs on World Map\n",
    "# Add a slight random variation to coordinates to make markers in high density areas (the bay) more legible \n",
    "\n",
    "#Init map with starting location at SF\n",
    "SF = [37.7749, -122.4194]\n",
    "USA = [40, -98]\n",
    "Zoom = 4\n",
    "CompanyMap = folium.Map(USA, tiles=\"cartodb positron\", zoom_start=Zoom)\n",
    "color_SU = \"darkblue\"\n",
    "color_LC = \"red\"\n",
    "color_F = \"green\"\n",
    "color_DC = \"purple\"\n",
    "\n",
    "for new_cords, new_CompName, new_Size, new_Focus, new_Founders \\\n",
    "    in  zip(coordinates_SU, data_SU['Company Name'], data_SU['Size'], \\\n",
    "            data_SU['Focus'],data_SU['CEO/Founders/Affliation']):  \n",
    "    variation = generate_random_pair()\n",
    "    folium.Marker(location=[a + b for a, b in zip(new_cords, variation)], \\\n",
    "                  tooltip=new_CompName,\n",
    "                  popup = new_CompName+'\\n Size: ' + str(new_Size) + \\\n",
    "                 '\\n Focus: ' + new_Focus + '\\n Key Members and Affliations: ' \\\n",
    "                 + new_Founders, \\\n",
    "                 icon=folium.Icon(icon=\"info-sign\", icon_color =\"white\", \\\n",
    "                                  color = color_SU)).add_to(CompanyMap)\n",
    "\n",
    "for new_cords, new_CompName, new_Size, new_Focus, new_Founders \\\n",
    "    in  zip(coordinates_LC, data_LC['Company Name'], data_LC['Size'], \\\n",
    "            data_LC['Focus'],data_LC['CEO/Founders/Affliation']):  \n",
    "    variation = generate_random_pair()\n",
    "    folium.Marker(location=[a + b for a, b in zip(new_cords, variation)], \\\n",
    "                  tooltip=new_CompName,\n",
    "                  popup = new_CompName+'\\n Size: ' + str(new_Size) + \\\n",
    "                 '\\n Focus: ' + new_Focus + '\\n Key Members and Affliations: ' \\\n",
    "                 + new_Founders, \\\n",
    "                 icon=folium.Icon(icon=\"info-sign\", icon_color =\"white\", color = color_LC)).add_to(CompanyMap)\n",
    "\n",
    "for new_cords, new_CompName, new_Size, new_Focus, new_Founders \\\n",
    "    in  zip(coordinates_F, data_F['Company Name'], data_F['Size'], \\\n",
    "            data_F['Focus'],data_F['CEO/Founders/Affliation']):  \n",
    "    variation = generate_random_pair()\n",
    "    folium.Marker(location=[a + b for a, b in zip(new_cords, variation)], \\\n",
    "                  tooltip=new_CompName,\n",
    "                  popup = new_CompName+'\\n Size: ' + str(new_Size) + \\\n",
    "                 '\\n Focus: ' + new_Focus + '\\n Key Members and Affliations: ' \\\n",
    "                 + new_Founders, \\\n",
    "                 icon=folium.Icon(icon=\"info-sign\", icon_color =\"white\", color = color_F)).add_to(CompanyMap)\n",
    "\n",
    "for new_cords, new_CompName, new_Size, new_Focus, new_Founders \\\n",
    "    in  zip(coordinates_DC, data_DC['Company Name'], data_DC['Size'], \\\n",
    "            data_DC['Focus'],data_DC['CEO/Founders/Affliation']):  \n",
    "    variation = generate_random_pair()\n",
    "    folium.Marker(location=[a + b for a, b in zip(new_cords, variation)], \\\n",
    "                  tooltip=new_CompName,\n",
    "                  popup = new_CompName+'\\n Size: ' + str(new_Size) + \\\n",
    "                 '\\n Focus: ' + new_Focus + '\\n Key Members and Affliations: ' \\\n",
    "                 + new_Founders, \\\n",
    "                 icon=folium.Icon(icon=\"info-sign\", icon_color =\"white\", color = color_DC)).add_to(CompanyMap)\n",
    "\n",
    "    \n",
    "minimap = plugins.MiniMap()\n",
    "CompanyMap.add_child(minimap)\n",
    "\n",
    "CompanyMap\n",
    "\n",
    "# ['red', 'blue', 'green', 'purple', 'orange', 'darkred',\n",
    "#  |           'lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue',\n",
    "#  |           'darkpurple', 'white', 'pink', 'lightblue', 'lightgreen',\n",
    "#  |           'gray', 'black', 'lightgray']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207063c-fbfe-4996-b6e5-6332644ca340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# help(folium.Icon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c55973-2acb-474e-b7f2-d20f5cb1ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
