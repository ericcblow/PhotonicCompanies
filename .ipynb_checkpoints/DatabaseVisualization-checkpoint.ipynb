{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726e61b1-f473-40a0-9bfe-b530b32d41f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009b3eb8-71e3-4518-bb4b-ed8e250ea480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_excel_data(file_path, sheet_name):\n",
    "    # Load the workbook and select the desired sheet\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook[sheet_name]\n",
    "\n",
    "    # Get labels from row 2 (B2 to G2)\n",
    "    labels = [sheet.cell(row=2, column=col).value for col in range(2, 7)]  # Columns B-G\n",
    "    # print(labels)\n",
    "    \n",
    "    # Check if all labels are present\n",
    "    if not all(labels):\n",
    "        raise ValueError(\"One or more labels in row 2 are missing.\")\n",
    "\n",
    "    # Initialize a dictionary to store the data for each label\n",
    "    data = {label: [] for label in labels}\n",
    "    \n",
    "    # Get data from rows starting at B3 (columns B-G)\n",
    "    for row in sheet.iter_rows(min_row=3, min_col=2, max_col=7, values_only=True):\n",
    "        for col_index, value in enumerate(row):\n",
    "            if value:  # Skip empty cells\n",
    "                data[labels[col_index]].append(value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d794b92f-ca18-4ae9-ac64-893162eedd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['LightMatter', 'Lightelligence', 'Ayar Labs', 'PsiQuantum', 'Quantum Transistors', 'Q.ANT', 'Lumai ', 'iPRONICS', 'Salience Labs', 'Pixel Photonics', 'Dream Photonics ', 'Akhetonics', 'Celestial AI', 'Xscape Photonics', 'Optelligence', 'Nubis Communications', 'Voyant Photonics', 'Aegiq', 'Nu-quantum', 'Miraex', 'Nordic Quantum Computing Group', 'Orca Computing', 'Photonic Inc', 'Quandela ', 'Quix', 'Sparrow Quantum', 'Lumiphase', 'Enosemi', 'Nexus Photonics', 'Black Semiconductor'], 'Size': [200, 200, '200+', 280, 50, 100, 20, '< 50', '< 50', '< 50', 10, '< 50', '< 200', '< 50', '< 3', '< 50 ', '< 30', '< 50 ', '< 50 ', 50, 'CLOSED 11/2024', '~100', '< 50', '< 50', '< 50', '< 50 ', '< 100 ', '< 50 ', '< 20 ', '<200'], 'Focus': ['Optical MVM / PNN', 'Optical MVM / Networking', 'Optical I/O for AI, Interconnection ', 'Quantum Computing / Optical Interconnect', 'solid-state\\xa0quantum\\xa0processor', 'Optical MVM', '3D optical AI computing ', 'Microwave Photonics and Networking', 'silicon photonic solutions that will solve the challenge the growth in AI', 'Superconducting Nanowire Single Photon Detectors (SNSPD) and Quantum Computing', 'Packaging and Layout Design; world’s best chipsets for integrated photonics', 'Neuromorphic, Quantum, Analog, Digital, ADC', 'Photonic Fabric (Networking for AI)', 'photonics solutions that enable the next generation of AI, ML, and simulation hardware.', 'Photonic AI ', 'Co-Packaged Optics, Pluggable Transceivers, Active Optical Cables', 'LiDAR', 'Full Stack Quantum Computing', 'Photonic-Enabled Quantum Computing', 'photonic and quantum solutions for next-generation sensing, networking and computing', 'CLOSED 11/2024', 'error-corrected photonic quantum computers', 'Silicon photonic Quantum Computing', 'Silicon photonic Quantum Computing', 'photonic quantum processor based upon silicon nitride waveguides', 'photonic quantum technology components', 'Optical interconnects and computing using Si Ph and BTO', 'Photonic-Electronic Design IP and Custom Silicon', 'Advanced heterogeneous integration, SiN, GaAs, GaN, InP', '2D material graphene to create ultra-fast, energy-efficient, and scalable chip fabrics'], 'CEO/Founders/Affliation': ['Nick Harris, Dirk Englund, David Miller, MIT, Stanford', 'Yichen Shen, MIT ', 'Mark Wade', \"Jeremy O'Brien;\\xa0Terry Rudolph;\\xa0Peter Shadbolt; Mark Thompson\", 'Shmuel Bachinsky', 'Michael Förtsch, TRUMPF Photonics', 'James Spall, University of Oxford', 'Jose Capmany', 'Oxford, Harish Bhaskaran and Wolfram Pernice', 'Wolfram Pernice', 'Lukas Chrostowski', 'David Lazovsky', 'Vivek Raghunathan, Michal Lipson, Keren Bergman, Columbia', 'Volker J. Sorger', 'Peter Winzer, Bell Labs', 'Steven Miller, Michel Lipson', 'Scott Dufferwiel, Univ. of Sheffield', 'Dr. Carmen Palacios-Berraquero, University of Cambridge', 'Nicolas Abelé, Karel Dumon and Clément Javerzac-Galy, Miraex, EPFL ', 'CLOSED 11/2024', 'Professor Ian Walmsley, Josh Nunn, Richard Murray, University of Oxford', 'Paul Terry, Stephanie Simmons, Simon Fraser University', 'Centre for Nanoscience and Nanotechnology', 'Hans van den Vlekkert, University of Twente', 'Peter Lodahl, Niels Bohr Institute Quantum Photonics Lab', 'Stefan Abel, BTO Modulator Author,  started by IBM Si Ph people', 'Matt Streshinsky, Came out of Lumious Computing'], 'Location': ['Mountain View, CA and Toronto, Canada, and Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel and NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancover, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, Toronto, Austin', 'Vancouver, Canada', 'Massy, Île-de-France', 'Ulm, Germany and Stuttgart Germany, and Amsterdam, Netherlands, and Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Startup'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_SU = import_excel_data(file_path, sheet_name)\n",
    "    print(data_SU)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992f9d63-498d-4af5-965d-36ef0ca6c7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Nvidia', 'Google ', 'NEC', 'Intel ', 'Universal Display Corporation', 'OpenAI', 'Apple', 'AMD', 'Cisco', 'Luxtera', 'Infinera ', 'NeoPhotonics', 'Coherent Inc. ', 'Hamamatsu Corporation', 'M Squared\\xa0', 'NTT', 'IBM', 'Synopsys Photonic Solutions'], 'Size': [500, 100], 'Focus': ['quantum technology, biophotonics, and chemical sensing'], 'CEO/Founders/Affliation': ['GRAEME MALCOLM OBE, Strathclyde University'], 'Location': ['NJ, USA and Japan', 'Glasgow and London and Palo Alto and  Boston and Berlin', 'Zurich, Switzerland', 'Mountain View, California']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Large Company'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_LC = import_excel_data(file_path, sheet_name)\n",
    "    print(data_LC)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba18db43-69da-46a7-a641-94178ecdc28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Global Foundries', 'AIM ', 'AMF ', 'HHI ', 'Smart Photonics', 'LionX', 'Tower', 'LIGENTEC', 'VTT', 'IMEC'], 'Size': [], 'Focus': [], 'CEO/Founders/Affliation': ['Michael Geiselmann, Michael Zervas,  Tobias Kippenberg,  EPFL '], 'Location': ['Vaud, Switzerland']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Foundries'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_F = import_excel_data(file_path, sheet_name)\n",
    "    print(data_F)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979d4eba-50eb-4cd6-a425-53bcc01d1a43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company Name': ['Bascom Hunter', 'Photonic Systems Inc', 'Phase Sensitive Innovations', 'Honeywell'], 'Size': ['< 100', '< 100', '< 100'], 'Focus': ['Microwave Photonics, Neuromorphioc Photonics', 'Microwave Photonics, Interference Cancellation', 'Microwave Photonics, Diffractive Optics, Phase Arrays, Thin Film LiNo'], 'CEO/Founders/Affliation': ['Paul R. Prucnal , Andy McCandless', 'Charles Cox, Edward Ackerman', 'Dennis Prather, Univ of Delware '], 'Location': ['Baton Rouge, LA']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = './Photonic Companies.xlsx'  # Replace with your Excel file path\n",
    "sheet_name = 'Defense Contractors'  # Replace with the name of the sheet\n",
    "\n",
    "try:\n",
    "    data_DC = import_excel_data(file_path, sheet_name)\n",
    "    print(data_DC)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8ce73-2611-4a6e-8617-63be6401238b",
   "metadata": {},
   "source": [
    "### If a company has multiple locations splits that into new rows and then duplicate the information to the new rows, the deliminator is \" and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6312641-e84e-440c-8091-21ce1e81f0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword = \" and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb27b6ad-39ac-47f6-b998-7ceb3c7b4da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search and split\n",
    "def search_and_split(Locations, keyword):\n",
    "    results = []\n",
    "    split_strings = []\n",
    "    NewLocations = []\n",
    "    Indx2Dup = []\n",
    "    j=0\n",
    "    NewLocations = Locations\n",
    "    for idx, string in enumerate(Locations):\n",
    "        # print(idx)\n",
    "        start_idx = string.find(keyword)\n",
    "        if start_idx != -1:  # Keyword found\n",
    "            end_idx = start_idx + len(keyword) - 1\n",
    "            results.append((idx, start_idx, end_idx))\n",
    "\n",
    "            # Split the string into two parts: before and after the keyword\n",
    "            before_keyword = string[:start_idx].strip()\n",
    "            after_keyword = string[end_idx + 1:].strip()\n",
    "            split_strings.append((before_keyword, after_keyword))\n",
    "            \n",
    "            # Append and replace new split string into original list\n",
    "            # Conditionals are to look for blank spaces or punctions and not include them \n",
    "            threshold = 2\n",
    "            if len(before_keyword) > threshold and len(after_keyword) > threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = before_keyword\n",
    "                NewLocations.insert(idx+1, after_keyword)\n",
    "                #idx is no longer real as New List grows, need to keep track\n",
    "                Indx2Dup.append(idx-j)\n",
    "                j+=1\n",
    "                #When duplicating the rows if there are consecutive number dup from \n",
    "                    # row - number of consecutive numbers\n",
    "                \n",
    "            if len(before_keyword) <= threshold and len(after_keyword) > threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = after_keyword\n",
    "                \n",
    "            if len(before_keyword) > threshold and len(after_keyword) <= threshold:\n",
    "                # print(len(NewLocations))\n",
    "                # print(idx)\n",
    "                NewLocations[idx] = before_keyword            \n",
    "\n",
    "                \n",
    "    return results, split_strings, NewLocations, Indx2Dup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78446a3-10b3-474e-abdd-6d908001967a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate: \n",
    "def duplicate_values_with_repeated_indices(indices, source_list):\n",
    "    # Create a new list to store the results\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(source_list)):\n",
    "        count = indices.count(i)  # Count occurrences of the index in the indices array\n",
    "        result.extend([source_list[i]] * (count + 1))  # Original + count of duplicates\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2906f6-1311-40e9-88d3-64fdfdb9eede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location List:\n",
      "['Mountain View, CA and Toronto, Canada, and Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel and NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancover, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, Toronto, Austin', 'Vancouver, Canada', 'Massy, Île-de-France', 'Ulm, Germany and Stuttgart Germany, and Amsterdam, Netherlands, and Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n",
      "30\n",
      "\n",
      "New Location List:\n",
      "['Mountain View, CA', 'Toronto, Canada,', 'Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel', 'NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancover, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, Toronto, Austin', 'Vancouver, Canada', 'Massy, Île-de-France', 'Ulm, Germany', 'Stuttgart Germany,', 'Amsterdam, Netherlands,', 'Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n",
      "36\n",
      "\n",
      "Index to Duplicate\n",
      "[0, 0, 4, 24, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "# Testing new search and split function\n",
    "# strings = [\n",
    "#     \"This is a sample string and it contains and perhaps denver and.\",\n",
    "#     \"Another example without the keyword.\",\n",
    "#     \"and is a conjunction.\",\n",
    "#     \"Nothing here or here\",\n",
    "#     \"But maybe here and there\"\n",
    "# ]\n",
    "\n",
    "# # Search for the keyword \"and\" and split strings\n",
    "# keyword = \"and\" # and is in Netherlands hahaha\n",
    "# matches, splits, nLocations, Indx = search_and_split(strings, keyword)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Keyword Matches (Start and End Indices):\")\n",
    "# for match in matches:\n",
    "#     print(f\"String index: {match[0]}, Start index: {match[1]}, End index: {match[2]}\")\n",
    "\n",
    "# print(\"\\nSplit Strings:\")\n",
    "# for idx, (before, after) in enumerate(splits):\n",
    "#     print(f\"Original String Index: {matches[idx][0]}\")\n",
    "#     print(f\"Before: '{before}'\")\n",
    "#     print(f\"After: '{after}'\")\n",
    "    \n",
    "# # print(\"\\nResult\")\n",
    "# # print(nLocations)\n",
    "\n",
    "print(\"\\nLocation List:\")\n",
    "print(data_SU[\"Location\"])\n",
    "print(len(data_SU[\"Location\"]))\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_SU[\"Location\"], keyword)\n",
    "\n",
    "print(\"\\nNew Location List:\")\n",
    "print(nLocations)\n",
    "print(len(nLocations))\n",
    "\n",
    "print(\"\\nIndex to Duplicate\")\n",
    "print(Indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06029d19-a774-4f3c-afa3-9ead4e68642e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_SU.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc5aa8e8-dfe5-4d9a-8693-4d272c9d1271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# indices = [1, 1, 3, 3, 3, 5]  # Indices for duplication\n",
    "# source_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "\n",
    "# # Duplicate values based on repeated indices\n",
    "# result = duplicate_values_with_repeated_indices(indices, source_list)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19957a0b-a496-4960-b870-9b55a4b7229f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate values at the specified indices for start up tab\n",
    "data_SU['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_SU['Company Name'])\n",
    "data_SU['Size'] = duplicate_values_with_repeated_indices(Indx, data_SU['Size'])\n",
    "data_SU['Focus'] = duplicate_values_with_repeated_indices(Indx, data_SU['Focus'])\n",
    "data_SU['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_SU['CEO/Founders/Affliation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0defbef2-4e1a-4e3b-b81b-96cc1065b9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same for LC, F, and DC\n",
    "matches, splits, nLocations, Indx = search_and_split(data_LC[\"Location\"], keyword)\n",
    "data_LC['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_LC['Company Name'])\n",
    "data_LC['Size'] = duplicate_values_with_repeated_indices(Indx, data_LC['Size'])\n",
    "data_LC['Focus'] = duplicate_values_with_repeated_indices(Indx, data_LC['Focus'])\n",
    "data_LC['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_LC['CEO/Founders/Affliation'])\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_F[\"Location\"], keyword)\n",
    "data_F['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_F['Company Name'])\n",
    "data_F['Size'] = duplicate_values_with_repeated_indices(Indx, data_F['Size'])\n",
    "data_F['Focus'] = duplicate_values_with_repeated_indices(Indx, data_F['Focus'])\n",
    "data_F['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_F['CEO/Founders/Affliation'])\n",
    "\n",
    "matches, splits, nLocations, Indx = search_and_split(data_DC[\"Location\"], keyword)\n",
    "data_DC['Company Name'] = duplicate_values_with_repeated_indices(Indx, data_DC['Company Name'])\n",
    "data_DC['Size'] = duplicate_values_with_repeated_indices(Indx, data_DC['Size'])\n",
    "data_DC['Focus'] = duplicate_values_with_repeated_indices(Indx, data_DC['Focus'])\n",
    "data_DC['CEO/Founders/Affliation'] = duplicate_values_with_repeated_indices(Indx, data_DC['CEO/Founders/Affliation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7aa1c6-862d-4ae8-8d51-4d8f0a0aa367",
   "metadata": {},
   "source": [
    "### Now I have corrected list for companies with multiple location, we convert rough individual locations into coordinates and make a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32be4a0-c69c-4e06-aba2-0fd2dea1ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Rough Location to longitude and latitude\n",
    "\n",
    "def get_coordinates_with_duplicates(locations):\n",
    "    # Initialize the geolocator\n",
    "    geolocator = Nominatim(user_agent=\"location_to_coordinates\")\n",
    "    cache = {}  # Cache to store already-queried locations\n",
    "    results = []  # Final list of results\n",
    "\n",
    "    for location in locations:\n",
    "        if location in cache:\n",
    "            # Use cached result if available\n",
    "            results.append(cache[location])\n",
    "        else:\n",
    "            try:\n",
    "                # Geocode the location\n",
    "                loc = geolocator.geocode(location)\n",
    "                if loc:\n",
    "                    coordinates = (loc.latitude, loc.longitude)\n",
    "                else:\n",
    "                    coordinates = None\n",
    "                # Cache the result and append to results\n",
    "                cache[location] = coordinates\n",
    "                results.append(coordinates)\n",
    "            except GeocoderTimedOut:\n",
    "                # Handle timeout gracefully\n",
    "                results.append(None)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd54006c-3a85-4831-87c3-379a8021ed94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mountain View, CA', 'Toronto, Canada,', 'Boston, MA', 'Boston, MA', 'San Jose, CA', 'Palo Alto, CA', 'Tel Aviv, Israel', 'NYC, US', 'Stuttgart, Germany', 'Oxford, England', 'Valencia, Spain', 'Oxford, England', 'Germany', 'Vancover, BC, Canada', 'Berlin, Germany', 'Santa Clara, CA, USA', 'NY, USA', 'Austin, USA', 'Murray Hill, New Jersey, USA', 'New York, NY 10018', 'Sheffield, England', 'Cambridge, UK', 'Écublens, Vaud, Switzerland', 'Oslo, Norway ', 'London, Toronto, Austin', 'Vancouver, Canada', 'Massy, Île-de-France', 'Ulm, Germany', 'Stuttgart Germany,', 'Amsterdam, Netherlands,', 'Enschede, Netherlands', 'Copenhagen, Denmark', 'Zurich, Switzerland', 'California,USA', 'Goleta, California, USA', 'Aachen, Germany']\n"
     ]
    }
   ],
   "source": [
    "print(data_SU[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c080c33-590b-43a9-ae4c-2fdccd367aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get coordinates from location list\n",
    "coordinates_SU = get_coordinates_with_duplicates(data_SU[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7cce5d-0bd6-44cb-9b75-de31b60a5f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates_LC = get_coordinates_with_duplicates(data_LC[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faf6b945-f03f-4a8c-aa48-d931d3c27410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates_F = get_coordinates_with_duplicates(data_F[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37da191-1e4b-4ca7-a16f-6a21a1fcdcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates_DC = get_coordinates_with_duplicates(data_DC[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e2fec-2db1-4da2-aea3-f944f5c75a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example list of locations\n",
    "# locations = [\n",
    "#     \"Boston, MA, USA\",\n",
    "#     \"New York, NY, USA\",\n",
    "#     \"Boston, MA, USA\",  # Duplicate\n",
    "#     \"San Francisco, CA, USA\",\n",
    "#     \"New York, NY, USA\"  # Duplicate\n",
    "# ]\n",
    "\n",
    "# # Get coordinates\n",
    "# coordinates = get_coordinates_with_duplicates(locations)\n",
    "\n",
    "# # Print results\n",
    "# for location, coord in zip(locations, coordinates):\n",
    "#     if coord:\n",
    "#         print(f\"{location}: Latitude = {coord[0]}, Longitude = {coord[1]}\")\n",
    "#     else:\n",
    "#         print(f\"{location}: Coordinates not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b9cb8-a276-43dd-a1ea-a3fbc8d6734b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
